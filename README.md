# DeepLearningFinalProject

# Window-Based Activation Functions

## Introduction
Deep learning draws analogies from the  human brain. One important analogy is how  neurons  transmit  information.   Biologically,  neurons transmit electric pulses through  “synapses”  between  adjacent  cells.

An  electric pulse will only be forwarded by a neuron  if  the  voltage on the receiving neuron  reaches  a  threshold  voltage,  called  “action  potential”.   Neurons  are  interconnected, and  multiple  neural  paths  can  converge  into  a single post-synaptic  neuron’s  action  potential.
This is a direct analogy with how deep learning models use a similar neuron model to transmit information, and use activation functions to determine relevant data. Perhaps the most common and simplest activation function is ReLU.  Due to its effectiveness, a lot of activation functions have been modeled after ReLU, essentially becoming fine-tuned variations of it (LeakyReLU, GELU, etc.).

We believe it’s a good idea to branch off from the element-wise implementation of activation functions; drawing inspiration from the brain, we present a set of window-based activation functions, which aim to resemble some of the biological neural network interactions.


## Related Work


### Datasets


## Architectures


## Experiments 

Table template
| Activation Function | Test Acc  |  |
| --- | --- | --- |
| ... | ... | ... |




## Results

## References

-
